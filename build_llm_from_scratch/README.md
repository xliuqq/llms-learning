# 从零构建大模型

## 1. 理解大语言模型

大语言模型在理解、生成和解释人类语言方面拥有出色的能力，但对“理解”能力，指LLM可以处理和生成看似连贯且符合语境的文本，但并不意味着真的拥有像人类一样的意识或理解能力。

早期的自然语言处理模型通常是为特定任务(如文本分类、语言翻译等)而设计的，大语言模型在各种自然语言处理任务中展现了更广泛的能力。

下一单词预测任务合理利用语言本身具有顺序这一特定进行模型训练。

研究表明，针对特定领域或任务量身打造的大语言模型在性能上往往优于ChatGPT等为多种应用场景而设计的通用大语言模型，如金融领域/医学领域的大模型。

大语言模型的构建通常包括***预训练***和***微调***两个步骤。
- 预训练：在大模型、多样化的数据集上进行训练，形成全面的语言模型，具备如文本补全、少样本推理等基本能力；
- 微调:在规模较小的特定任务或领域数据集上对模型进行针对性训练（如分类、总结、翻译等），提升其特定能力；

预训练的数据也称为**原始文本**，没有标注信息，通常会进行数据过滤如删除格式字符串或未知语言的文档。预训练出来的模型称为**基础模型**，如ChatGPT前身的GPT-3。

微调大预言模型的两种方位是**指令微调**和**分类任务**微调：
- 指令微调: 标注数据集由”指令-答案“对组成，如翻译任务中的”原文-译文“；
- 分类任务微调: 标注数据集由文本及其类别标签组成，如标记为“垃圾”和“非垃圾的”邮件文本。
  
Transformer 最初是为机器翻译任务开发的，架构上由**编码器**和**解码器**构成。
- 编码器：将输入文本，编码为数值向量表示，捕捉上下文信息；
- 解码器：将编码后的向量表示，解码为输出文本。

编码器和解码器都由多层组成，这些层通过自注意力机制连接。**自注意力机制**衡量序列中不同单词或词元之间的相对重要性，因此能够捕捉输入数据中的长距离依赖和上下文关系。

BERT 基于编码器构建，专注于**掩码预测**，即预测给定句子中被掩码的词。主要用于文本分类等任务。

GPT 基于解码器构建，主要用于处理文本生成/补全的任务。

> GPT 擅长做生成容易理解，但为什么 BERT 擅长做分类？

词元是模型读取文本的基本单位，GPT-3 在3000 亿个词元上进行训练。训练数据集的庞大规模和丰富多样性使得这些模型在包括语言语法、语义、上下文，甚至一些需要通用知识的任务上，都拥有良好的表现。

纯解码器架构的GPT模型，旨在预测下一个单词，但也可以执行翻译任务，模型能够完成未经明确训练的任务的能力称为**涌现**，是其广泛接触大量多语言数据和各种上下文的自然结果。

## 2. 处理文本数据

**嵌入（embedding）**是指将数据（离散对象）转换为向量格式的过程。不同的数据格式需要使用不同的嵌入模型，例如为文本设计的嵌入模型并不适合嵌入音频或视频数据。

存在针对句子、段落甚至整个文档的嵌入技术，在**检索增强生成（retrieval-augmented generation）**领域非常流行。通过将生成（如生成文本)与检索（如搜索外部知识库)相结合，检索增强生成能够在生成过程中引入与上下文有关的外部信息。

词嵌入的最早流行的方法是 word2vec，通过神经网络架构的训练，能够根据目标词预测上下文，或者根据上下文预测目标词，并生成词嵌入。其核心思想是，出现在相似上下文中的词往往具有相似的含义（投影到二维并可视化）。

大语言模型通常会自行生成嵌入，这些嵌入是输入层的一部分。与 word2vec 相比，将嵌入作为大预言模型训练的一部分，嵌入可以针对特定的任务和数据进行优化（如与上下文相关的输入嵌入)。GPT-3模型（175B参数)的嵌入维度为 12288。

文本分词是将输入的文本分割为独立的词元（以下针对英文）：
- 可以以空白符、标点符号等作为分隔符，使用正则表达式；保留大写形式有助于模型区分专有名词和普通名词，理解句子结构以及正确地生成大写字母；
- 构建词元<->ID的双向映射，用于编码和解码；引入新词元`<|unk|>`和`<|endoftext|>`，分别表示未知词元和两段文本间的分隔符；
- 不同的 LLM 中可能使用 `[BOS]`(序列开始)、`[EOS]`(序列结束)、`[PAD]`(填充)等特殊词元。

BPE （Byte-Pair Encoding）：将频繁出现的字符合并为子词，再将频繁出现的子词合并为单词，迭代构建词汇表。
- Python 开源库 `tiktoken`基于 Rust 高效实现分词推理（提供教育版的BPE训练代码）；

训练数据构建流程：
- 编码成词元：`toktoken`中可以下载多个预训练好的编码器，如`gpt2`、`cl100k_base`等；
- 通过上下文长度、步幅，将输入文本的词元向量，建立输入数据列表（每行是一个上下文词元ID的向量）和预测数据列表：
- 通过嵌入层将每个词元 ID 转换为 N 维的嵌入向量，嵌入层权重的维度为 V & N, 其中 V 是词元数量，N 是嵌入维度。
  - 嵌入层实质上执行的是一次查找操作，词元 ID 为 3 的应用结果，等价于嵌入层权重的第 3 行（索引都从  0 开始)；
- 嵌入向量加入位置编码信息，不同位置的词元 ID 的嵌入向量不同；
  - 绝对位置嵌入：与序列中的特定位置相关联，为每个位置分配一个唯一的编码，学习法或者固定法(《Attention Is All You Need》首次提出的正弦位置编码）；
  - 相对位置嵌入：词元之间的相对位置或距离，能够使模型更好适应不同的长度（包括训练时未遇过的长度)；
  - 旋转位置编码：既能够在词向量中嵌入绝对位置信息，又能在经过注意力机制计算后可以反映出相对位置信息；
  

## 3. 编码自注意力机制

Transformer 之前，长序列建模中，循环神经网络(recurrent neural network, RNN)是语言翻译中最流行的编码器-解码器架构。
- 编码器将整个输入文本处理成一个隐藏状态（记忆单元），解码器使用这个隐藏单元来生成输出；
- 限制：解码阶段，RNN 无法直接访问编码器中的早期隐藏状态，只能依赖当前的隐藏状态，可能导致复杂句子中的上下文丢失；
- 基于RDD的Bahdanau注意力机制（2014年）：解码器在每个解码步骤中，可以选择性地访问输入序列的不同部分。

在计算序列表示时，自注意力机制允许**输入序列中的每个位置关注同一序列中的所有位置**。
- “自”：通过关联单个输入序列中的不同位置来计算注意力权重的能力。传统的注意力机制关注的两个不同（如输入/输出）的序列元素之间的关系。

自注意力机制的目标是为每个输入元素计算一个上下文向量，该向量结合了其他所有输入元素的信息。
- 对输入序列中的每个元素 $x^{(i)}$ 计算上下文向量 $z^{(i)}$（包含了序列中所有元素信息的嵌入向量）； 
- 注意力分数 $\omega_{i,j}=x^{i}*x^{j}$：每个输入词元 $x^{(i)}$ 与其他所有输入词元的点积（得到向量，再softmax归一化处理)；
- 上下文向量 $z^{(i)}=\sum_{j=0}^{n}{\omega_{i,j}x^{(j)}}$

可训练的权重矩阵 $W_q, W_k, W_v$：
- 用于将嵌入的词元向量$x^{(i)}$映射到查询向量 $q^{(i)}$、键向量 $k^{(i)}$ 和值向量 $v^{(i)}$；
- 未缩放的注意力分数: 查询向量和键向量的点积；
- 注意力权重: 缩放的注意力分数并应用softmax（将注意力分数除以**键向量的嵌入维度的平方根**来缩放)；
- 上下文向量：对值向量及逆行加权求和计算（注意力权重作为加权因子，衡量每个值向量的重要性）；

> 查询：代表模型当前关注或者试图理解的项（如句中的一个单词或词元）；
> 键：用于跟查询进行匹配，输入序列中每个项（如句中每个单词）都有一个对应的键；
> 值：输入项的实际内容或表示。

因果注意力掩码：预测下一个单词，当前的词元应该只能看到前面的词元，不应该获取后面未知的信息（不等同于BERT是掩码)。
1. 对注意力权重掩码对角线以上的元素（设为0）：通过 tril 生成掩码矩阵，再执行按位乘法；后面的词元没有概率影响前面的词元；
2. 再重新归一化注意力权重，使得每行的总和为 1（ $x_i = x_i / sum(x)$）；
3. （1）和（2）等价于对未缩放的注意力分数，将对角线以上部分用 $-\infin$ 填充（$e^{-\infin}=0$），再进行 softmax 归一化；

Dropout 掩码额外的注意力权重，减少过拟合： dropout 仅在训练期间使用，训练结束后会被取消；
- Transformer 中通常有两处应用：一是计算注意力权重之后，而是将这些权重应用于值向量之后。

多头注意力：“多头”是将注意力机制分成多个“头”，每个“头”独立工作。因果注意力扩展到多头注意力：
- 直接堆叠多个 CausalAttention 模块（层），每个实例都有独立的参数，最终得到多个上下文向量，进行拼接成一个更长的上下文向量；
- 或**使用矩阵乘法的同时计算所有注意力头**的输出（大语言模型架构中，默认对多头注意力上下文加量，后面增加一个线性投影层）；

> 最小的 GPT-2 模型（参数量为 1.17 亿）有 12 个注意力头，上下文嵌入向量维度为 768。
> 最大的 GPT-2 模型（参数量为 15 亿）有 25 个注意力头，上下文嵌入向量维度为 1600。

