# 从零构建大模型

## 1. 理解大语言模型

大语言模型在理解、生成和解释人类语言方面拥有出色的能力，但对“理解”能力，指LLM可以处理和生成看似连贯且符合语境的文本，但并不意味着真的拥有像人类一样的意识或理解能力。

早期的自然语言处理模型通常是为特定任务(如文本分类、语言翻译等)而设计的，大语言模型在各种自然语言处理任务中展现了更广泛的能力。

下一单词预测任务合理利用语言本身具有顺序这一特定进行模型训练。

研究表明，针对特定领域或任务量身打造的大语言模型在性能上往往优于ChatGPT等为多种应用场景而设计的通用大语言模型，如金融领域/医学领域的大模型。

大语言模型的构建通常包括***预训练***和***微调***两个步骤。
- 预训练：在大模型、多样化的数据集上进行训练，形成全面的语言模型，具备如文本补全、少样本推理等基本能力；
- 微调:在规模较小的特定任务或领域数据集上对模型进行针对性训练（如分类、总结、翻译等），提升其特定能力；

预训练的数据也称为**原始文本**，没有标注信息，通常会进行数据过滤如删除格式字符串或未知语言的文档。预训练出来的模型称为**基础模型**，如ChatGPT前身的GPT-3。

微调大预言模型的两种方位是**指令微调**和**分类任务**微调：
- 指令微调: 标注数据集由”指令-答案“对组成，如翻译任务中的”原文-译文“；
- 分类任务微调: 标注数据集由文本及其类别标签组成，如标记为“垃圾”和“非垃圾的”邮件文本。
  
Transformer 最初是为机器翻译任务开发的，架构上由**编码器**和**解码器**构成。
- 编码器：将输入文本，编码为数值向量表示，捕捉上下文信息；
- 解码器：将编码后的向量表示，解码为输出文本。

编码器和解码器都由多层组成，这些层通过自注意力机制连接。**自注意力机制**衡量序列中不同单词或词元之间的相对重要性，因此能够捕捉输入数据中的长距离依赖和上下文关系。

BERT 基于编码器构建，专注于**掩码预测**，即预测给定句子中被掩码的词。主要用于文本分类等任务。

GPT 基于解码器构建，主要用于处理文本生成/补全的任务。

> GPT 擅长做生成容易理解，但为什么 BERT 擅长做分类？

词元是模型读取文本的基本单位，GPT-3 在3000 亿个词元上进行训练。训练数据集的庞大规模和丰富多样性使得这些模型在包括语言语法、语义、上下文，甚至一些需要通用知识的任务上，都拥有良好的表现。

纯解码器架构的GPT模型，旨在预测下一个单词，但也可以执行翻译任务，模型能够完成未经明确训练的任务的能力称为**涌现**，是其广泛接触大量多语言数据和各种上下文的自然结果。

## 2. 处理文本数据



